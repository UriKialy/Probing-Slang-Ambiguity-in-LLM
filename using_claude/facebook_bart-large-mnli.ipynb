{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T22:01:29.639105Z",
     "start_time": "2025-06-03T22:00:12.991485Z"
    }
   },
   "source": [
    "# ## 1. Install Required Packages (uncomment if needed)\n",
    "# !pip install transformers torch pandas tqdm scikit-learn\n",
    "\n",
    "# ## 2. Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ## 3. Load the Dataset\n",
    "data_path = r\"C:\\Users\\yozev\\PycharmProjects\\Probing-Slang-Ambiguity-in-LLM\\using_claude\\manual_slang_dataset.csv\"\n",
    "df = pd.read_csv(data_path).rename(columns={\"sentence\": \"text\", \"binary\": \"label\"})\n",
    "print(\"Dataset size:\", len(df))\n",
    "display(df.head())\n",
    "\n",
    "# ## 4. Initialize Tokenizer and Model (Pure PyTorch NLI)\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ## 5. Zero-Shot Prediction Function\n",
    "def zero_shot_predict(sentences, batch_size=16):\n",
    "    \"\"\"\n",
    "    For each sentence in `sentences`, computes:\n",
    "      P_entail(slang) vs. P_entail(literal)\n",
    "    using the NLI model. Whichever entailment probability is higher\n",
    "    becomes the predicted label.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Zero-shot batches\"):\n",
    "        batch_texts = sentences[i : i + batch_size]\n",
    "\n",
    "        # Prepare two hypotheses for each text\n",
    "        hyp_slang = [\"This example is slang.\" for _ in batch_texts]\n",
    "        hyp_literal = [\"This example is literal.\" for _ in batch_texts]\n",
    "\n",
    "        # Tokenize premise–hypothesis pairs\n",
    "        enc_slang = tokenizer(batch_texts, hyp_slang, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        enc_literal = tokenizer(batch_texts, hyp_literal, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Move to device\n",
    "        enc_slang = {k: v.to(device) for k, v in enc_slang.items()}\n",
    "        enc_literal = {k: v.to(device) for k, v in enc_literal.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            out_slang = model(**enc_slang)\n",
    "            out_literal = model(**enc_literal)\n",
    "\n",
    "        # Logits shape: (batch_size, 3) → [contradiction, neutral, entailment]\n",
    "        # We take the entailment probability (index 2)\n",
    "        probs_slang = torch.softmax(out_slang.logits, dim=1)[:, 2]\n",
    "        probs_literal = torch.softmax(out_literal.logits, dim=1)[:, 2]\n",
    "\n",
    "        # Compare entailment probabilities for each example\n",
    "        for ps, pl in zip(probs_slang.cpu(), probs_literal.cpu()):\n",
    "            if ps > pl:\n",
    "                predictions.append(\"slang\")\n",
    "            else:\n",
    "                predictions.append(\"literal\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# ## 6. Run Zero-Shot Classification on Entire Dataset\n",
    "sentences = df[\"text\"].tolist()\n",
    "true_labels = [\"slang\" if lab == 1 else \"literal\" for lab in df[\"label\"].tolist()]\n",
    "\n",
    "predicted_labels = zero_shot_predict(sentences, batch_size=16)\n",
    "\n",
    "# ## 7. Compute Accuracy\n",
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"\\nZero-Shot NLI Accuracy: {acc:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  That new song is absolutely fire, I can't stop...      1\n",
       "1  The fire department responded quickly to the h...      0\n",
       "2  Your outfit is so bad, everyone's going to be ...      1\n",
       "3   I got a bad grade on my chemistry test yesterday      0\n",
       "4   She killed that performance, the crowd went wild      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That new song is absolutely fire, I can't stop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The fire department responded quickly to the h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your outfit is so bad, everyone's going to be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got a bad grade on my chemistry test yesterday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She killed that performance, the crowd went wild</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yozev\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Zero-shot batches: 100%|██████████| 46/46 [01:14<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot NLI Accuracy: 0.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "5447d8bce1395614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
